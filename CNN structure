pip install tensorflow
pip install Spectral
from tensorflow.keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, BatchNormalization,
Dropout, Input, AveragePooling2D, Activation, GlobalAveragePooling2D, add, concatenate,
MaxPooling3D, MaxPooling2D
from tensorflow.keras.models import Model, Sequential, load_model
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau, CSVLogger,
EarlyStopping
# from keras.utils import np_utils
from tensorflow.keras import utils
from tensorflow.keras.regularizers import l2

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report,
cohen_kappa_score, hamming_loss, jaccard_score, log_loss, roc_curve, auc

from operator import truediv

from plotly.offline import init_notebook_mode

import numpy as np
import matplotlib.pyplot as plt
import scipy.io as sio
import os
from spectral import imshow as spyShow
from spectral import spy_colors

from matplotlib import patches
import pandas as pd
import tensorflow as tf
from clr_callback import CyclicLR

import tkinter as tk
from tkinter.ttk import *
import glob
path = os.getcwd()
def merging(rows,cols,path):
labels = sio.loadmat(path+&#39;\\label final.mat&#39;)[&#39;a&#39;]
cwd= path+&#39;\\s1&#39;
lines= open(cwd+&#39;\\s1Common.txt&#39;,&#39;r&#39;).read().split(&#39;\n&#39;)
arr_merged= np.zeros(shape=(int(rows),int(cols),len(lines)))
c=0

# names_ea= [&#39;/Stokes1_A new.mat&#39;,&#39;/Stokes1_phi new.mat&#39;,&#39;/lambda new.mat&#39;] #these .mat files
are related to ALOS PALSAR 1.1 satellite data
# key=[&#39;a&#39;]
# for i in range(len(names_ea)):
# total.append((sio.loadmat(os.getcwd()+names_ea[i])[key[0]][0:size,0:size]))
# print(len(total))

lines = [&#39;/Intensity_VV_blue.mat&#39; , &#39;/Intensity_VV_green.mat&#39; , &#39;/Intensity_VV_red.mat&#39;]

for i in range(len(lines)-1):
x= sio.loadmat(path+&#39;\\features\\&#39;+lines[i])[&#39;A&#39;]
x= np.array(x)

if x.ndim==2:
arr_merged[:,:,c]=x[0:rows,0:cols]
c+= 1
if x.ndim==3:
for y in range(x.shape[2]):
arr_merged[:,:,y]=x[0:rows,0:cols,y]
c+=1
return arr_merged, labels

def padWithZeros(X, margin=2):
newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))
x_offset = margin
y_offset = margin
newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X
return newX

def createImageCubes(X, y, windowSize=9, removeZeroLabels = True):
margin = int((windowSize - 1) / 2)
zeroPaddedX = padWithZeros(X, margin=margin)
# split patches
patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))
patchesLabels = np.zeros((X.shape[0] * X.shape[1]))
patchIndex = 0
for r in range(margin, zeroPaddedX.shape[0] - margin):
for c in range(margin, zeroPaddedX.shape[1] - margin):
patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]
patchesData[patchIndex, :, :, :] = patch

patchesLabels[patchIndex] = y[r-margin, c-margin]
patchIndex = patchIndex + 1
if removeZeroLabels:
patchesData = patchesData[patchesLabels&gt;0,:,:,:]
patchesLabels = patchesLabels[patchesLabels&gt;0]
patchesLabels -= 1
return patchesData, patchesLabels

def splitTrainTestSet(X, y, testRatio, randomState=42):
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio,
random_state=randomState)
print(&quot;X train {}\n X test {}\n Y train{}\n Y test {}\n&quot;.format(X_train[0],X_test[0], y_train[0],
y_test[0]))
return X_train, X_test, y_train, y_test

def remove_nan(X):
where_are_NaNs = np.isnan(X)
where_are_inf = np.isinf(X)
X[where_are_NaNs] = 1e-6
X[where_are_inf] = 1e-6
# print(np.where(where_are_NaNs==True))
# print(np.min(X[:,:,0]))
# putting the 3 channels back together:
t=np.zeros((np.shape(X)[0], np.shape(X)[1]), dtype=np.float64)
for i in range(X.shape[2]):
t=X[:,:,i]
#t=(t - np.min(t)) / (np.max(t) - np.min(t))
# print(t)

#X[:,:,i]=t

# for i in range(x.shape[2]):
# t=x[:,:,i]
# t = (t - np.mean(t)) / np.std(t)
# print(t)
# X[:,:,i]=t
where_are_NaNs = np.isnan(X)
where_are_inf = np.isinf(X)
X[where_are_NaNs] = 1e-6
X[where_are_inf] = 1e-6
return X

# #test-train split
def training_testing_validation(X,y,val_ratio,test_ratio):
Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(np.array((X)), np.array((y)), test_ratio)
Xtrain, Xvalid, ytrain, yvalid = splitTrainTestSet(Xtrain, ytrain, val_ratio)
print(&quot;Training data&quot;,Xtrain.shape)
ytrain = utils.to_categorical(ytrain)
print(&quot;Training GT&quot;,ytrain.shape)
print(&quot;Validation data&quot;,Xvalid.shape)
yvalid = utils.to_categorical(yvalid)
print(&quot;Validation GT&quot;,yvalid.shape)
return Xtrain,ytrain,Xtest,ytest,Xvalid,yvalid

def model_arch(windowSize,Xvalid,num_classes,epochs,name,blr,mlr):
# -------LE-Net 5---------

# S = windowSize
# L = Xvalid.shape[3]

# ## convolutional layers
# ## input layer
# input_layer = Input((S, S, L, 1))
# conv_layer1 = Conv3D(filters=6, padding=&quot;same&quot;, kernel_size=(5, 5, 5),
# kernel_initializer=&quot;VarianceScaling&quot;, kernel_regularizer=tf.keras.regularizers.l2(2e-4),
# activation=&#39;tanh&#39;)(input_layer)#7
# pool1 = MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2), padding=&quot;same&quot;)(conv_layer1)

# conv_layer2 = Conv3D(filters=16, padding=&quot;same&quot;, kernel_size=(5, 5, 5),
# kernel_initializer=&quot;VarianceScaling&quot;, kernel_regularizer=tf.keras.regularizers.l2(2e-4),
# activation=&#39;tanh&#39;)(pool1)#5
# pool2 = MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2), padding=&quot;same&quot;)(conv_layer2)

# conv_layer3 = Conv3D(filters=120, padding=&quot;same&quot;, kernel_size=(5, 5, 5),
# kernel_initializer=&quot;VarianceScaling&quot;, kernel_regularizer=tf.keras.regularizers.l2(2e-4),
# activation=&#39;tanh&#39;)(pool2)#3

# flatten_layer = Flatten()(conv_layer3)

# ## fully connected layers
# dense_layer1 = Dense(units=84, activation=&#39;tanh&#39;)(flatten_layer)
# dense_layer2 = Dense(units=10, activation=&#39;tanh&#39;)(dense_layer1)
# output_layer = Dense(units=num_classes, activation=&#39;softmax&#39;)(dense_layer2)

S = windowSize
L = Xvalid.shape[3]

## convolutional layers
## input layer
input_layer = Input((S, S, L, 1))
conv_layer1 = Conv3D(filters=16, padding=&quot;same&quot;, kernel_size=(5, 5, 5),
kernel_initializer=&quot;VarianceScaling&quot;, kernel_regularizer=tf.keras.regularizers.l2(2e-4),
activation=&#39;elu&#39;)(input_layer)#7
conv_layer1=BatchNormalization(epsilon=1e-03, momentum=0.9, weights=None)(conv_layer1)
conv_layer2 = Conv3D(filters=16, padding=&quot;same&quot;, kernel_size=(5, 5, 5),
kernel_initializer=&quot;VarianceScaling&quot;, kernel_regularizer=tf.keras.regularizers.l2(2e-4),
activation=&#39;elu&#39;)(conv_layer1)#5
conv_layer2=BatchNormalization(epsilon=1e-03, momentum=0.9, weights=None)(conv_layer2)
conv_layer3=add([conv_layer2,conv_layer1])
conv_layer4 = Conv3D(filters=32, padding=&quot;same&quot;, kernel_size=(5, 5, 5),
kernel_initializer=&quot;VarianceScaling&quot;, kernel_regularizer=tf.keras.regularizers.l2(2e-4),
activation=&#39;elu&#39;)(conv_layer3)#3
#conv_layer5=BatchNormalization(epsilon=1e-03, momentum=0.9, weights=None)(conv_layer4)
#conv_layer4=add([conv_layer3,conv_layer1])
conv3d_shape = conv_layer4.shape
conv_layer5 = Reshape((conv3d_shape[1], conv3d_shape[2],
conv3d_shape[3]*conv3d_shape[4]))(conv_layer4)
conv_layer6 = Conv2D(filters=32, padding=&quot;same&quot;, kernel_size=(3,3),
kernel_initializer=&quot;VarianceScaling&quot;,
kernel_regularizer=tf.keras.regularizers.l2(2e-4), activation=&#39;elu&#39;)(conv_layer5)

flatten_layer = Flatten()(conv_layer6)

## fully connected layers
dense_layer1 = Dense(units=256, activation=&#39;elu&#39;)(flatten_layer)
dense_layer1 = Dropout(0.3)(dense_layer1)
dense_layer2 = Dense(units=128, activation=&#39;elu&#39;)(dense_layer1)
dense_layer2 = Dropout(0.3)(dense_layer2)
output_layer = Dense(units=num_classes, activation=&#39;softmax&#39;)(dense_layer2)

# input_layer = Input((S, S, L, 1))

# conv_layer1 = Conv3D(filters=16, padding=&quot;same&quot;, kernel_size=(5, 5, 5),
activation=&#39;relu&#39;)(input_layer)#7
# # pool1 = MaxPooling3D(pool_size=(2, 2, 2), strides=None, padding=&quot;same&quot;)(conv_layer1)

# conv_layer2 = Conv3D(filters=24, padding=&quot;same&quot;, kernel_size=(5, 5, 5),
activation=&#39;relu&#39;)(conv_layer1)#5
# pool2 = MaxPooling3D(pool_size=(2, 2, 2), strides=None, padding=&quot;same&quot;)(conv_layer2)
# l2 = BatchNormalization()(pool2)

# conv_layer3 = Conv3D(filters=32, padding=&quot;same&quot;, kernel_size=(5, 5, 5), activation=&#39;relu&#39;)(l2)#3
# # pool3 = MaxPooling3D(pool_size=(2, 2, 2), strides=None, padding=&quot;same&quot;)(conv_layer3)

# conv3d_shape = conv_layer3.shape
# conv_layer3 = Reshape((conv3d_shape[1], conv3d_shape[2],
conv3d_shape[3]*conv3d_shape[4]))(conv_layer3)

# conv_layer4 = Conv2D(filters=64, padding=&quot;same&quot;, kernel_size=(5,5),
activation=&#39;relu&#39;)(conv_layer3)
# pool4 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=&quot;same&quot;)(conv_layer4)

# flatten_layer = Flatten()(pool4)

# ## fully connected layers
# dense_layer1 = Dense(units=512, activation=&#39;relu&#39;)(flatten_layer)
# dense_layer2 = Dense(units=256, activation=&#39;relu&#39;)(dense_layer1)
# dense_layer3 = Dense(units=64, activation=&#39;relu&#39;)(dense_layer2)

# dense_layer3 = Dropout(0.4)(dense_layer3)

# output_layer = Dense(units=num_classes, activation=&#39;softmax&#39;)(dense_layer3)

# _input_ = Input(shape=(S, S, L, 1))

# conv1 = Conv3D(filters=64, kernel_size=(3, 3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot; ,
kernel_initializer=&quot;VarianceScaling&quot;,kernel_regularizer=tf.keras.regularizers.l2(2e-4))(_input_)
# conv2 = Conv3D(filters=64, kernel_size=(3, 3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot; ,
kernel_initializer=&quot;VarianceScaling&quot;,kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv1)
# pool1 = MaxPooling3D(pool_size=(2, 2, 2), strides=None, padding=&quot;same&quot;)(conv2)
# l1 = BatchNormalization()(pool1)

# conv3 = Conv3D(filters=128, kernel_size=(3, 3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot; ,
kernel_initializer=&quot;VarianceScaling&quot;,kernel_regularizer=tf.keras.regularizers.l2(2e-4))(l1)
# conv4 = Conv3D(filters=128, kernel_size=(3, 3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot; ,
kernel_initializer=&quot;VarianceScaling&quot;,kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv3)
# pool2 = MaxPooling3D(pool_size = (2, 2, 2), strides=None, padding=&quot;same&quot;)(conv4)
# l2 = BatchNormalization()(pool2)

# conv5 = Conv3D(filters=256, kernel_size=(3, 3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot; ,
kernel_initializer=&quot;VarianceScaling&quot;,kernel_regularizer=tf.keras.regularizers.l2(2e-4))(l2)
# conv6 = Conv3D(filters=256, kernel_size=(3, 3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot; ,
kernel_initializer=&quot;VarianceScaling&quot;,kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv5)
# conv7 = Conv3D(filters=256, kernel_size=(3, 3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot;,
kernel_initializer=&quot;VarianceScaling&quot;,kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv6)
# pool3 = MaxPooling3D(pool_size = (2, 2, 2), strides=None, padding=&quot;same&quot;)(conv7)
# l3 = BatchNormalization()(pool3)

# conv8 = Conv3D(filters=512, kernel_size=(3,3,3), padding=&quot;same&quot;, activation=&quot;relu&quot; ,
kernel_initializer=&quot;VarianceScaling&quot;,kernel_regularizer=tf.keras.regularizers.l2(2e-4))(l3)
# conv9 = Conv3D(filters=512, kernel_size=(3,3,3), padding=&quot;same&quot;, activation=&quot;relu&quot; ,
kernel_initializer=&quot;VarianceScaling&quot;,kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv8)
# conv10 = Conv3D(filters=512, kernel_size=(3,3,3), padding=&quot;same&quot;, activation=&quot;relu&quot; ,
kernel_initializer=&quot;VarianceScaling&quot;,kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv9)
# pool4 = MaxPooling3D(pool_size = (2, 2, 2), strides=None, padding=&quot;same&quot;)(conv10)
# l4 = BatchNormalization()(pool4)

# conv11 = Conv3D(filters=512, kernel_size=(3,3,3), padding=&quot;same&quot;, activation=&quot;relu&quot; ,
kernel_initializer=&quot;VarianceScaling&quot;,kernel_regularizer=tf.keras.regularizers.l2(2e-4))(l4)
# conv12 = Conv3D(filters=512, kernel_size=(3,3,3), padding=&quot;same&quot;, activation=&quot;relu&quot; ,
kernel_initializer=&quot;VarianceScaling&quot;,kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv11)
# conv13 = Conv3D(filters=512, kernel_size=(3,3,3), padding=&quot;same&quot;, activation=&quot;relu&quot; ,
kernel_initializer=&quot;VarianceScaling&quot;,kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv12)
# pool5 = MaxPooling3D(pool_size = (2, 2, 2), strides=None, padding=&quot;same&quot;)(conv13)
# l5 = BatchNormalization()(pool5)

# flat = Flatten()(l5)
# dense1 = Dense(4096, activation=&quot;relu&quot;)(flat)
# dense2 = Dense(4096, activation=&quot;relu&quot;)(dense1)
# output = Dense(num_classes, activation=&quot;softmax&quot;)(dense2)

# vgg16_model = Model(inputs=_input_, outputs=output)
# model = vgg16_model

model = Model(inputs=input_layer, outputs=output_layer)

model.summary()

# tf.keras.utils.plot_model(model, show_shapes=True)

# Compile the Model
#model.compile(loss=&#39;categorical_crossentropy&#39;,
# optimizer= RMSprop(lr= 0.0001, epsilon=1e-08),
#optimizer= Adam(lr= 1e-2, epsilon=1e-08),
# optimizer = SGD(learning_rate=1e-4, momentum=0.9, nesterov=True),
#metrics=[&#39;accuracy&#39;])
model.compile(optimizer=Adam(lr= 1e-4, beta_1=0.9, beta_2=0.95, epsilon=1e-02),
loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

reduce_lr = ReduceLROnPlateau(monitor=&#39;val_loss&#39;, factor=np.sqrt(0.001), patience=5, verbose=1,
min_lr=0.001) #on plateaus

earlystop = EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=0.01, patience=10 ,verbose=1,
mode=&#39;auto&#39;)

checkpoint = ModelCheckpoint(name, verbose=1, save_best_only = True, monitor=&quot;val_loss&quot;)

#clr_triangular = CyclicLR(mode=&#39;exp_range&#39;, gamma=0.5)

clr_triangular = CyclicLR(base_lr=0.001, max_lr=0.006,step_size=2000,mode=&#39;exp_range&#39;,
gamma=0.8)

callbacks_list = [earlystop,reduce_lr,checkpoint,clr_triangular]
return model, callbacks_list

def train(model,callbacks_list,Xtrain,ytrain,Xvalid,yvalid):
history = model.fit(x=Xtrain, y=ytrain, validation_data = (Xvalid,yvalid[:,:,1]), batch_size=128,
epochs=epochs, callbacks=callbacks_list)
return history

def prediction(model,Xtest):
Y_pred_test = model.predict(Xtest)
y_pred_test = np.argmax(Y_pred_test, axis=1)
return y_pred_test, Y_pred_test

def plots(history):
plt.figure(figsize=(15,15))
plt.grid()
plt.xlabel(&#39;Number of epochs&#39;, fontsize=20)
plt.ylabel(&#39;MSE&#39;, fontsize=20)
plt.plot(history.history[&#39;loss&#39;], linewidth=5)
plt.plot(history.history[&#39;val_loss&#39;], linewidth=5)
plt.legend([&#39;Train Loss&#39;, &#39;Validation Loss&#39;], loc=&#39;upper right&#39;, fontsize=&#39;x-large&#39;)

plt.figure(figsize=(15,15))
plt.ylim(0,1.1)
plt.grid()

plt.xlabel(&#39;Number of epochs&#39;, fontsize=20)
plt.ylabel(&#39;Accuracy&#39;, fontsize=20)
plt.plot(history.history[&#39;accuracy&#39;], linewidth=5)
plt.plot(history.history[&#39;val_accuracy&#39;], linewidth=5)
plt.legend([&#39;Train Accuracy&#39;, &#39;Validation Accuracy&#39;], loc=&#39;upper right&#39;, fontsize=&#39;x-large&#39;)

def AA_andEachClassAccuracy(confusion_matrix):
counter = confusion_matrix.shape[0]
list_diag = np.diag(confusion_matrix)
list_raw_sum = np.sum(confusion_matrix, axis=1)
each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))
average_acc = np.mean(each_acc)
return each_acc, average_acc

def reports (X_test,y_test,name,model):
file_name = name+&#39;\\s1\\cropnet_report_s1.txt&#39;

Y_pred, y_pred= prediction(model,X_test)
print(&quot;Y_prediction&quot;,y_pred.shape)
print(&quot;y_test&quot;,np.argmax(y_test, axis=1).shape)
# loss_log= log_loss(y_pred, np.argmax(y_test, axis=1), np.arange(0,12,1))
# fpr = dict()
# tpr = dict()
# roc_auc = dict()
# for i in range(num_classes):
# fpr[i], tpr[i], _ = roc_curve(y_test[:, i], Y_pred[:, i])
# roc_auc[i] = auc(fpr[i], tpr[i])

# Plot of a ROC curve for a specific class
# for i in range(num_classes):
# plt.figure()
# plt.plot(fpr[i], tpr[i], label=&#39;ROC curve (area = %0.2f)&#39; % roc_auc[i])
# plt.plot([0, 1], [0, 1], &#39;k--&#39;)
# plt.xlim([0.0, 1.0])
# plt.ylim([0.0, 1.05])
# plt.xlabel(&#39;False Positive Rate&#39;)
# plt.ylabel(&#39;True Positive Rate&#39;)
# plt.title(&#39;Receiver operating characteristic example&#39;)
# plt.legend(loc=&quot;lower right&quot;)
# plt.show()
# roc_auc = roc_auc_score(y_test,Y_pred)
#end = time.time()
#print(end - start)
target_names = [ &#39;Onion&#39;, &#39;Potato&#39;]
# print(y_pred.shape)
# print(y_test.shape)
classification = classification_report(np.argmax(y_test, axis=1), Y_pred,
target_names=target_names)
oa = accuracy_score(np.argmax(y_test, axis=1), Y_pred)
confusion = confusion_matrix(np.argmax(y_test, axis=1), Y_pred)
each_acc, aa = AA_andEachClassAccuracy(confusion)
kappa = cohen_kappa_score(np.argmax(y_test, axis=1), Y_pred)
score = model.evaluate(X_test, y_test, batch_size=32)
# fpr, tpr, threshold = roc_curve(np.argmax(y_test, axis=1), y_pred, pos_label=1)
# roc_auc= auc(fpr, tpr)

ham_loss= hamming_loss(np.argmax(y_test,axis=1), Y_pred)
jac_score= jaccard_score(np.argmax(y_test,axis=1), Y_pred, average=None)
Test_Loss = score[0]*100
Test_accuracy = score[1]*100

if os.path.exists(file_name):
try:
os.remove(file_name)
except OSError:
pass
with open(file_name, &#39;w&#39;) as x_file:
x_file.write(&#39;Test loss {}(%)&#39;.format(Test_Loss))
x_file.write(&#39;\n \n&#39;)
x_file.write(&#39;Test accuracy {}(%)&#39;.format(Test_accuracy))
x_file.write(&#39;\n \n&#39;)
x_file.write(&#39;Kappa accuracy {}(%)&#39;.format(kappa))
x_file.write(&#39;\n \n&#39;)
x_file.write(&#39;Overall accuracy {}(%)&#39;.format(oa))
x_file.write(&#39;\n \n&#39;)
x_file.write(&#39;Average accuracy {}(%)&#39;.format(aa))
x_file.write(&#39;\n \n&#39;)
x_file.write(&#39;Confusion Matrix\n&#39;)
x_file.write(&#39;\n \n&#39;)
x_file.write(&#39;{}&#39;.format(classification))
x_file.write(&#39;\n \n&#39;)
x_file.write(&#39;{}&#39;.format(confusion))
x_file.write(&#39;\n \n&#39;)

x_file.write(&#39;Each class accuracy \t{}&#39;.format(each_acc))
x_file.write(&#39;\n \n&#39;)
x_file.write(&#39;Hamming loss \t{}(%)&#39;.format(ham_loss))
x_file.write(&#39;\n \n&#39;)
x_file.write(&#39;Jaccard score \t{}(%)&#39;.format(jac_score))
x_file.write(&#39;\n \n&#39;)
# x_file.write(&#39;Log loss \t{}(%)&#39;.format(loss_log))
# x_file.write(&#39;\n \n&#39;)
# x_file.write(&#39;False positive rates for each class is \t{}(%)&#39;.format(fpr))
# x_file.write(&#39;\n \n&#39;)
# x_file.write(&#39;True positive rates for each class is \t{}(%)&#39;.format(tpr))
# x_file.write(&#39;\n \n&#39;)
# x_file.write(&#39;Area under ROC curve is \t{}&#39;.format(roc_auc))
print(confusion)
print(&quot;Test loss and accuracy is&quot;,Test_Loss, Test_accuracy, ham_loss)
print(jac_score)
print(oa, each_acc, aa, kappa)

def Patch(data,height_index,width_index,windowSize):
PATCH_SIZE = windowSize
height_slice = slice(height_index, height_index+PATCH_SIZE)
width_slice = slice(width_index, width_index+PATCH_SIZE)
patch = data[height_slice, width_slice, :]
return patch

def visualize(height,width,X,y,model,name,windowSize):

outputs = np.zeros((height,width))
count=0;
for i in range(height):
for j in range(width):
target = int(y[i,j])
if target == 0 :
count+=1
continue
else :
image_patch=Patch(X,i,j,windowSize)
X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1],
image_patch.shape[2], 1).astype(&#39;float64&#39;)
# print(image_patch.shape[0],image_patch.shape[1],image_patch.shape[2])
prediction = (model.predict(X_test_image))
prediction = np.argmax(prediction, axis=1)
# print(prediction)
outputs[i][j] = prediction+1
count+=1
print(count)
print(outputs[i][j])

mdic = {&quot;outputs&quot;: outputs, &quot;label&quot;: &quot;predicted&quot;}
sio.savemat(name,mdic)

imageView = spyShow(classes=y, fignum=1 ,figsize =(15,15), interpolation=&#39;none&#39;)
# imageView.set_display_mode(&#39;overlay&#39;)
labelDictionary={0:&#39;Unknown&#39;, 1:&#39;Onion&#39;,2:&#39;Potato&#39;}

labelPatches = [ patches.Patch(color=spy_colors[x]/255.,
label=labelDictionary[x]) for x in np.unique(y) ]
plt.legend(handles=labelPatches, ncol=5, fontsize=&#39;medium&#39;,
loc=&#39;upper left&#39;,bbox_to_anchor=(0.5, -0.05))

predict_image = spyShow(classes = outputs.astype(int),figsize =(15,15))
plt.legend(handles=labelPatches, ncol=5, fontsize=&#39;medium&#39;,
loc=&#39;upper left&#39;,bbox_to_anchor=(0.5, -0.05))

if __name__==&#39;__main__&#39;:
#declaring the various parameters of the network
w_size= 5
rows=3229
cols= 19
test_train_ratio= 0.10
test_val_ratio= 0.20
epochs= 300
blr= 1e-4
mlr= 1e-3
X,y= merging(rows,cols,os.getcwd())

#removing any nan or infy values
X= remove_nan(X)

print(X.shape)
print(np.unique(y))

num_classes= len(np.unique(y))-1

#creating small cubes from the 3d matrix
X, y = createImageCubes(X, y, w_size)
print(X.shape, y.shape)
#test-train split
Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(X, y, test_train_ratio)
print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)

Xtrain = Xtrain.reshape(-1, w_size, w_size, Xtrain.shape[3], 1)
ytrain = utils.to_categorical(ytrain)
Xtrain, Xvalid, ytrain, yvalid = splitTrainTestSet(Xtrain, ytrain, test_val_ratio)
print(Xtrain.shape, Xvalid.shape, ytrain.shape, yvalid.shape)

Xvalid = Xvalid.reshape(-1, w_size, w_size, Xtrain.shape[3], 1)
print(Xvalid.shape)

yvalid = utils.to_categorical(yvalid)
print(yvalid[0:20,:,1]) #first 20 rows,all cols, 2nd channel

model,callbacks_list=
model_arch(w_size,Xvalid,num_classes,epochs,os.getcwd()+&#39;\\s1\\cropnets1.hdf5&#39;,blr,mlr)
history= train(model,callbacks_list,Xtrain,ytrain,Xvalid,yvalid)

plots(history)

model = load_model(os.getcwd()+&#39;\\s1\\cropnets1.hdf5&#39;, custom_objects={&#39;tf&#39;: tf})

Xtest = Xtest.reshape(-1, w_size, w_size, Xtrain.shape[3], 1)
print(Xtest.shape)
ytest = utils.to_categorical(ytest)
print(ytest.shape)

reports(Xtest,ytest,os.getcwd(),model)

X, y = merging(rows,cols,os.getcwd())
X= remove_nan(X)
height = y.shape[0]
width = y.shape[1]
PATCH_SIZE = w_size
#numComponents =
print(height,width)
# X,pca = applyPCA(X, numComponents=numComponents)
X = padWithZeros(X, PATCH_SIZE//2)
visualize(height,width,X,y,model,os.getcwd()+&#39;\\s1\\cropnet_predicted_s1.mat&#39;,w_size)
